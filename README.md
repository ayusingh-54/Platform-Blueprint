# Pulsepoint: AI-Powered E-Commerce Control Tower - Complete Development Guide

## Overview

Pulsepoint is a **production-grade MCP (Model Context Protocol) server** that unifies e-commerce operations by connecting Shopify inventory with multi-channel advertising data (Meta, TikTok, Google). Using Claude AI, it generates intelligent recommendations to optimize ad spend while maintaining inventory balance.

**Key Value Proposition:**

- ğŸ¯ **Attribute orders to campaigns** with multi-source data fusion
- ğŸ“¦ **Prevent stockouts** by matching inventory with ad spend
- ğŸ’° **Maximize profitability** through AI-driven recommendations
- ğŸ“Š **Complete visibility** into what works and why
- ğŸ” **Enterprise-ready** multi-tenant architecture with full data isolation
- âš¡ **n8n Integration** - Automated workflow orchestration
- ğŸ“ˆ **Real-time Observability** - Token tracking and cost analysis
- ğŸ”„ **Multi-destination Events** - PostgreSQL, Pub/Sub, BigQuery

---

## Table of Contents

1. [Phase 1: Foundation Setup](#phase-1-foundation-setup)
2. [Phase 2: Core Services](#phase-2-core-services)
3. [Phase 3: Data Integration](#phase-3-data-integration)
4. [Phase 4: AI & Intelligence](#phase-4-ai--intelligence)
5. [Phase 5: Multi-Tenancy & Observability](#phase-5-multi-tenancy--observability)
6. [Phase 6: n8n Automation](#phase-6-n8n-automation)
7. [Phase 7: Deployment](#phase-7-deployment)

---

## Phase 1: Foundation Setup

### Step 1.1: Environment & Dependencies

**What to do first:** Set up your local development environment

```bash
# 1. Create project directory
mkdir pulsepoint && cd pulsepoint
git init

# 2. Create Python virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Create requirements.txt with dependencies
cat > requirements.txt << 'EOF'
fastapi==0.104.1
uvicorn==0.24.0
sqlalchemy==2.0.23
asyncpg==0.29.0
alembic==1.12.1
pydantic==2.5.0
python-dotenv==1.0.0
aioredis==2.0.1
anthropic==0.7.0
google-cloud-bigquery==3.13.1
google-cloud-pubsub==2.18.4
google-cloud-storage==2.10.0
requests==2.31.0
httpx==0.25.0
cryptography==41.0.7
pyjwt==2.8.1
pytest==7.4.3
pytest-asyncio==0.21.1
black==23.11.0
ruff==0.1.6
n8n-integration==0.1.0  # n8n Python SDK
EOF

pip install -r requirements.txt

# 4. Create environment file
cat > .env.example << 'EOF'
# Database
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/pulsepoint
REDIS_URL=redis://localhost:6379/0

# Google Cloud
GCP_PROJECT_ID=your-project-id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json

# API Keys
SHOPIFY_API_KEY=your-shopify-key
SHOPIFY_API_SECRET=your-shopify-secret
META_ACCESS_TOKEN=your-meta-token
TIKTOK_ACCESS_TOKEN=your-tiktok-token
GOOGLE_ADS_DEVELOPER_TOKEN=your-google-token
ANTHROPIC_API_KEY=your-claude-key

# JWT
JWT_SECRET=your-secret-key
JWT_ALGORITHM=HS256
JWT_EXPIRY_HOURS=24

# Server
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
ENVIRONMENT=development

# n8n
N8N_URL=http://localhost:5678
N8N_WEBHOOK_URL=http://localhost:5678/webhook
EOF

cp .env.example .env
# Edit .env with your actual credentials
```

### Step 1.2: Project Structure

**Create folder structure:**

```bash
# Backend structure
mkdir -p backend/{api,services,integrations,models,database,utils,jobs}
mkdir -p backend/api/{routes,middleware}
mkdir -p backend/database/migrations
mkdir -p backend/tests/{unit,integration,e2e}

# Frontend structure
mkdir -p frontend/src/{components,services,pages,styles}

# Scripts
mkdir -p scripts

# Documentation
mkdir -p docs
```

### Step 1.3: Database Setup

**Create PostgreSQL database:**

```bash
# 1. Create database
createdb pulsepoint

# 2. Install PostgreSQL extensions
psql pulsepoint << 'EOF'
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";
EOF

# 3. Initialize Alembic for migrations
alembic init backend/database/migrations

# 4. Run initial migrations (created in Step 2.1)
alembic upgrade head
```

---

## Phase 2: Core Services

### Step 2.1: Database Models & RLS

**File: `backend/models/database.py`**

```python
from sqlalchemy import Column, String, UUID, DateTime, Boolean, Enum, Integer
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import declarative_base
from datetime import datetime
import uuid

Base = declarative_base()

class Tenant(Base):
    __tablename__ = "tenants"
    __table_args__ = {"schema": "public"}

    tenant_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    status = Column(String(20), default="active")  # active, suspended, deleted
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class Brand(Base):
    __tablename__ = "brands"

    brand_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    tenant_id = Column(UUID(as_uuid=True), nullable=False)
    name = Column(String(255), nullable=False)
    shopify_store = Column(String(255))
    status = Column(String(20), default="active")
    created_at = Column(DateTime, default=datetime.utcnow)

class AITokenUsage(Base):
    __tablename__ = "ai_token_usage"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    tenant_id = Column(UUID(as_uuid=True), nullable=False)
    brand_id = Column(UUID(as_uuid=True))
    model = Column(String(100), default="claude-3-sonnet")
    operation = Column(String(100))  # ai_generation, analysis, etc.
    input_tokens = Column(Integer, default=0)
    output_tokens = Column(Integer, default=0)
    total_tokens = Column(Integer, default=0)
    total_cost_usd = Column(Float, default=0.0)
    recorded_at = Column(DateTime, default=datetime.utcnow)

class User(Base):
    __tablename__ = "users"

    user_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    tenant_id = Column(UUID(as_uuid=True), nullable=False)
    email = Column(String(255), nullable=False)
    role = Column(String(50), default="user")  # admin, manager, user
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
```

**File: `scripts/setup_db.py` - Enable RLS:**

```python
import asyncio
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine

async def setup_rls():
    engine = create_async_engine("postgresql+asyncpg://...")

    async with engine.begin() as conn:
        # Enable RLS on all tables
        await conn.execute(text("""
            ALTER TABLE tenants ENABLE ROW LEVEL SECURITY;
            ALTER TABLE brands ENABLE ROW LEVEL SECURITY;
            ALTER TABLE users ENABLE ROW LEVEL SECURITY;
            ALTER TABLE ai_token_usage ENABLE ROW LEVEL SECURITY;
        """))

        # Create tenant isolation policy
        await conn.execute(text("""
            CREATE POLICY tenant_isolation ON brands
            USING (tenant_id = current_setting('app.tenant_id')::uuid);
        """))

        await conn.commit()
        print("âœ“ RLS policies created")

asyncio.run(setup_rls())
```

**Run setup:**

```bash
python scripts/setup_db.py
```

### Step 2.2: FastAPI Application Setup

**File: `backend/api/main.py`**

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging

# Import middleware and routes
from backend.api.middleware.tenant_context import TenantContextMiddleware
from backend.api.routes import brands, questions, admin

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("ğŸš€ Starting Pulsepoint MCP Server")
    yield
    logger.info("ğŸ›‘ Shutting down Pulsepoint")

app = FastAPI(
    title="Pulsepoint MCP Server",
    description="AI-powered e-commerce control tower",
    version="1.0.0",
    lifespan=lifespan
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Custom middleware
app.add_middleware(TenantContextMiddleware)

# Routes
app.include_router(brands.router, prefix="/api/brands", tags=["Brands"])
app.include_router(questions.router, prefix="/api/questions", tags=["Q&A"])
app.include_router(admin.router, prefix="/api/admin", tags=["Admin"])

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "pulsepoint-mcp"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Run server:**

```bash
python -m uvicorn backend.api.main:app --reload
```

### Step 2.3: Authentication & JWT

**File: `backend/services/auth_service.py`**

```python
import jwt
from datetime import datetime, timedelta
from typing import Optional
import os

class AuthService:
    SECRET = os.getenv("JWT_SECRET")
    ALGORITHM = os.getenv("JWT_ALGORITHM", "HS256")

    @classmethod
    def create_token(cls, tenant_id: str, user_id: str, role: str) -> str:
        payload = {
            "sub": user_id,
            "tenant_id": tenant_id,
            "role": role,
            "exp": datetime.utcnow() + timedelta(hours=24)
        }
        return jwt.encode(payload, cls.SECRET, algorithm=cls.ALGORITHM)

    @classmethod
    def verify_token(cls, token: str) -> dict:
        try:
            payload = jwt.decode(token, cls.SECRET, algorithms=[cls.ALGORITHM])
            return payload
        except jwt.ExpiredSignatureError:
            raise ValueError("Token expired")
        except jwt.InvalidTokenError:
            raise ValueError("Invalid token")
```

---

## Phase 3: Data Integration

### Step 3.1: Shopify Integration

**File: `backend/integrations/shopify_service.py`**

```python
import httpx
from typing import List
import os

class ShopifyService:
    def __init__(self, api_key: str, api_secret: str):
        self.api_key = api_key
        self.api_secret = api_secret
        self.base_url = "https://{shop}.myshopify.com/admin/api/2024-01"

    async def get_products(self, shop_name: str) -> List[dict]:
        """Fetch all products from Shopify"""
        url = f"{self.base_url}/products.json".format(
            shop=shop_name
        )
        headers = {
            "X-Shopify-Access-Token": self.api_key
        }

        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=headers)
            return response.json()["products"]

    async def get_orders(self, shop_name: str, status: str = "any") -> List[dict]:
        """Fetch orders from Shopify"""
        url = f"{self.base_url}/orders.json?status={status}".format(
            shop=shop_name
        )
        headers = {"X-Shopify-Access-Token": self.api_key}

        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=headers)
            return response.json()["orders"]
```

### Step 3.2: Meta Ads Integration

**File: `backend/integrations/meta_ads_service.py`**

```python
import httpx

class MetaAdsService:
    def __init__(self, access_token: str):
        self.access_token = access_token
        self.base_url = "https://graph.facebook.com/v18.0"

    async def get_campaign_metrics(self, account_id: str) -> dict:
        """Fetch campaign performance metrics"""
        url = f"{self.base_url}/{account_id}/campaigns"
        params = {
            "access_token": self.access_token,
            "fields": "id,name,status,spend,impressions,clicks,actions"
        }

        async with httpx.AsyncClient() as client:
            response = await client.get(url, params=params)
            return response.json()
```

### Step 3.3: Background Job Scheduler

**File: `backend/jobs/sync_shopify.py`**

```python
import asyncio
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

class ShopifySyncJob:
    def __init__(self, shopify_service, db_session):
        self.shopify_service = shopify_service
        self.db_session = db_session

    async def run(self, tenant_id: str, brand_id: str):
        """Sync Shopify data for brand"""
        try:
            logger.info(f"Syncing Shopify data for brand {brand_id}")

            # Get products
            products = await self.shopify_service.get_products()

            # Get orders from last 24 hours
            orders = await self.shopify_service.get_orders()

            # Store in database
            await self._store_products(tenant_id, brand_id, products)
            await self._store_orders(tenant_id, brand_id, orders)

            logger.info(f"âœ“ Sync completed: {len(products)} products, {len(orders)} orders")

        except Exception as e:
            logger.error(f"âœ— Sync failed: {str(e)}")
            raise

    async def _store_products(self, tenant_id, brand_id, products):
        # Store in database
        pass

    async def _store_orders(self, tenant_id, brand_id, orders):
        # Store in database
        pass
```

---

## Phase 4: AI & Intelligence

### Step 4.1: Claude AI Integration

**File: `backend/integrations/anthropic_service.py`**

```python
from anthropic import Anthropic
import json

class AnthropicService:
    def __init__(self, api_key: str):
        self.client = Anthropic(api_key=api_key)
        self.model = "claude-3-5-sonnet-20241022"

    async def analyze_performance_data(self, data: dict, question: str) -> str:
        """Use Claude to analyze e-commerce data"""

        context = f"""
        You are an e-commerce analytics expert. Analyze the following data and answer the question.

        Data:
        {json.dumps(data, indent=2)}

        Question: {question}

        Provide actionable insights and recommendations.
        """

        message = self.client.messages.create(
            model=self.model,
            max_tokens=1024,
            messages=[
                {"role": "user", "content": context}
            ]
        )

        return message.content[0].text

    async def generate_recommendations(self, metrics: dict) -> list:
        """Generate AI recommendations for ad spend allocation"""

        prompt = f"""
        Based on these metrics, provide 5 specific recommendations to improve ROI:
        {json.dumps(metrics, indent=2)}

        Format as JSON array with fields: [priority, action, expected_impact]
        """

        message = self.client.messages.create(
            model=self.model,
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )

        return json.loads(message.content[0].text)
```

### Step 4.2: Recommendation Engine

**File: `backend/services/recommendation_engine.py`**

```python
class RecommendationEngine:
    def __init__(self, ai_service, db_session):
        self.ai_service = ai_service
        self.db_session = db_session

    async def generate_budget_recommendations(
        self,
        tenant_id: str,
        brand_id: str
    ) -> list:
        """Generate ad spend recommendations"""

        # Fetch current metrics
        metrics = await self._get_brand_metrics(brand_id)

        # Use Claude AI to generate recommendations
        recommendations = await self.ai_service.generate_recommendations(metrics)

        # Store in database
        await self._store_recommendations(tenant_id, brand_id, recommendations)

        return recommendations

    async def _get_brand_metrics(self, brand_id: str) -> dict:
        # Query database for brand metrics
        return {}

    async def _store_recommendations(self, tenant_id, brand_id, recommendations):
        # Store recommendations in database
        pass
```

---

## Phase 5: Multi-Tenancy & Observability

### Step 5.1: Tenant Context Middleware

**File: `backend/api/middleware/tenant_context.py`**

```python
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
import jwt
from contextvars import ContextVar

# Thread-safe context variables
current_tenant_id = ContextVar('tenant_id', default=None)
current_user_id = ContextVar('user_id', default=None)

class TenantContextMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # Extract JWT token
        auth_header = request.headers.get("Authorization")
        if not auth_header:
            return {"error": "Missing authorization header"}

        try:
            token = auth_header.replace("Bearer ", "")
            payload = jwt.decode(token, "secret", algorithms=["HS256"])

            # Set context
            current_tenant_id.set(payload.get("tenant_id"))
            current_user_id.set(payload.get("sub"))

            # Store in request state for database queries
            request.state.tenant_id = payload.get("tenant_id")
            request.state.user_id = payload.get("sub")

        except jwt.InvalidTokenError:
            return {"error": "Invalid token"}

        response = await call_next(request)
        return response
```

### Step 5.2: Observability Service

**File: `backend/services/observability_service.py`**

```python
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import uuid

class EventCategory(str, Enum):
    API_LIFECYCLE = "api.lifecycle"
    AI_OPERATIONS = "ai.operations"
    BILLING = "billing"
    SECURITY = "security"
    DATA_SYNC = "data.sync"

class EventSeverity(str, Enum):
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class StructuredEvent:
    event_id: str = None
    request_id: str = None
    tenant_id: str = None
    category: EventCategory = None
    event_type: str = None
    severity: EventSeverity = None
    timestamp: datetime = None
    tokens_input: int = 0
    tokens_output: int = 0
    cost_usd: float = 0.0
    duration_ms: float = 0.0

    def __post_init__(self):
        if not self.event_id:
            self.event_id = str(uuid.uuid4())
        if not self.timestamp:
            self.timestamp = datetime.utcnow()

class ObservabilityService:
    def __init__(self, db_session, bigquery_client):
        self.db_session = db_session
        self.bigquery_client = bigquery_client

    async def track_ai_generation(
        self,
        tenant_id: str,
        tokens_input: int,
        tokens_output: int,
        generation_time_ms: float
    ) -> StructuredEvent:
        """Track AI API usage"""

        # Calculate cost: Claude Sonnet pricing
        cost = (tokens_input * 3 + tokens_output * 15) / 1_000_000

        event = StructuredEvent(
            tenant_id=tenant_id,
            category=EventCategory.AI_OPERATIONS,
            event_type="ai.answer_generated",
            severity=EventSeverity.INFO,
            timestamp=datetime.utcnow(),
            tokens_input=tokens_input,
            tokens_output=tokens_output,
            cost_usd=cost,
            duration_ms=generation_time_ms
        )

        # Store in PostgreSQL for compliance
        await self._store_in_postgres(event)

        # Publish to BigQuery for analytics
        await self._publish_to_bigquery(event)

        return event

    async def _store_in_postgres(self, event: StructuredEvent):
        # Store in database
        pass

    async def _publish_to_bigquery(self, event: StructuredEvent):
        # Publish to BigQuery
        pass
```

---

## Phase 6: n8n Automation

### Step 6.1: n8n Setup & Installation

**n8n runs as a separate service. Install via Docker:**

```bash
# 1. Create n8n directory
mkdir -p n8n-data

# 2. Start n8n with Docker
docker run -it --rm \
  -p 5678:5678 \
  -e N8N_HOST=localhost \
  -e N8N_PROTOCOL=http \
  -v n8n-data:/home/node/.n8n \
  n8nio/n8n

# Access at http://localhost:5678
```

### Step 6.2: n8n Webhook Integration

**File: `backend/api/routes/webhooks.py` - Receive n8n events:**

```python
from fastapi import APIRouter, Request
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

@router.post("/webhooks/n8n/data-sync")
async def handle_n8n_sync(request: Request):
    """
    Receives webhook from n8n when data sync completes
    n8n sends: {"tenant_id", "brand_id", "sync_type", "records_count"}
    """
    payload = await request.json()

    logger.info(f"n8n sync webhook: {payload}")

    tenant_id = payload.get("tenant_id")
    brand_id = payload.get("brand_id")
    sync_type = payload.get("sync_type")  # shopify, meta, tiktok
    record_count = payload.get("records_count")

    # Trigger recommendations generation
    recommendations = await generate_new_recommendations(
        tenant_id, brand_id
    )

    return {
        "status": "processed",
        "recommendations": len(recommendations)
    }

@router.post("/webhooks/n8n/ai-analysis")
async def handle_n8n_ai_analysis(request: Request):
    """
    Receives AI analysis results from n8n
    """
    payload = await request.json()

    # Store analysis results
    await store_analysis(payload)

    return {"status": "stored"}
```

### Step 6.3: n8n Workflow Templates

**Workflow 1: Hourly Data Sync**

```
Trigger (Cron: Every Hour)
  â†“
Get Tenant Configuration (Database)
  â†“
Parallel Branch:
  â”œâ”€ Sync Shopify Data
  â”œâ”€ Sync Meta Ads
  â”œâ”€ Sync TikTok Ads
  â””â”€ Sync Google Ads
  â†“
Combine Results
  â†“
Send Webhook to Pulsepoint
  â†“
Log to PostgreSQL
```

**Workflow 2: Daily AI Analysis**

```
Trigger (Cron: Daily at 2 AM)
  â†“
Get Yesterday's Metrics
  â†“
Call Pulsepoint API (/api/analyze)
  â†“
Claude AI Analysis (via Pulsepoint)
  â†“
Generate Recommendations
  â†“
Send Email Alerts
  â†“
Store Results in BigQuery
```

**Workflow 3: Real-time Alert Trigger**

```
Webhook Listener (Shopify Stockout Event)
  â†“
Check Ad Spend Status
  â†“
If spend > threshold:
  â”œâ”€ Generate Alert
  â”œâ”€ Send Slack Message
  â””â”€ Trigger Recommendation
  â†“
Webhook back to Pulsepoint
```

### Step 6.4: n8n Environment Setup

**Create `.n8n/workflows/config.json`:**

```json
{
  "workflows": [
    {
      "name": "Hourly Data Sync",
      "enabled": true,
      "trigger": "cron",
      "cron": "0 * * * *",
      "webhook": "http://localhost:8000/webhooks/n8n/data-sync"
    },
    {
      "name": "Daily AI Analysis",
      "enabled": true,
      "trigger": "cron",
      "cron": "0 2 * * *",
      "webhook": "http://localhost:8000/webhooks/n8n/ai-analysis"
    }
  ]
}
```

### Step 6.5: n8n Credentials Configuration

**In n8n UI:**

1. **HTTP Request Node:**

   - Pulsepoint API URL: `http://localhost:8000/api`
   - Auth: Bearer Token (from Pulsepoint)

2. **Database Node:**

   - PostgreSQL connection details
   - Database: pulsepoint

3. **Slack Node** (for alerts):

   - Slack webhook URL

4. **Gmail Node** (for email alerts):
   - Gmail credentials

---

## Phase 7: Deployment

### Step 7.1: Docker Setup

**File: `Dockerfile`**

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY backend/ backend/
COPY .env .env

# Run migrations
RUN alembic upgrade head

# Start server
CMD ["uvicorn", "backend.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**File: `docker-compose.yml`**

```yaml
version: "3.8"

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: pulsepoint
      POSTGRES_PASSWORD: secure_password
      POSTGRES_DB: pulsepoint
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  pulsepoint-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql+asyncpg://pulsepoint:secure_password@postgres:5432/pulsepoint
      REDIS_URL: redis://redis:6379/0
    depends_on:
      - postgres
      - redis

  n8n:
    image: n8nio/n8n
    ports:
      - "5678:5678"
    environment:
      N8N_HOST: localhost
      N8N_PROTOCOL: http
    volumes:
      - n8n_data:/home/node/.n8n

volumes:
  postgres_data:
  n8n_data:
```

**Deploy:**

```bash
docker-compose up -d
```

### Step 7.2: GCP Cloud Run Deployment

**File: `scripts/deploy.sh`**

```bash
#!/bin/bash

# Set project ID
PROJECT_ID="your-gcp-project"
SERVICE_NAME="pulsepoint-api"
REGION="us-central1"

# Build Docker image
docker build -t gcr.io/$PROJECT_ID/$SERVICE_NAME:latest .

# Push to Container Registry
docker push gcr.io/$PROJECT_ID/$SERVICE_NAME:latest

# Deploy to Cloud Run
gcloud run deploy $SERVICE_NAME \
  --image gcr.io/$PROJECT_ID/$SERVICE_NAME:latest \
  --platform managed \
  --region $REGION \
  --allow-unauthenticated \
  --set-env-vars="DATABASE_URL=$DATABASE_URL,REDIS_URL=$REDIS_URL"

echo "âœ“ Deployed to Cloud Run"
echo "Service URL: https://$SERVICE_NAME-xxxxx.a.run.app"
```

**Run deployment:**

```bash
chmod +x scripts/deploy.sh
./scripts/deploy.sh
```

### Step 7.3: Monitoring & Observability

**BigQuery Dashboard Query:**

```sql
-- Daily costs by tenant
SELECT
  DATE(timestamp) as date,
  tenant_id,
  SUM(cost_usd) as daily_cost,
  SUM(tokens_input) as input_tokens,
  SUM(tokens_output) as output_tokens,
  AVG(duration_ms) as avg_latency_ms,
  COUNT(*) as api_calls
FROM `project.pulsepoint.observability_events`
WHERE timestamp >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
GROUP BY date, tenant_id
ORDER BY date DESC, daily_cost DESC;
```

---

## Complete Development Workflow

### Recommended Development Order:

1. **Week 1: Foundation** (Phase 1-2)

   - Setup environment âœ“
   - Create database models âœ“
   - Setup FastAPI âœ“
   - Implement authentication âœ“

2. **Week 2: Integrations** (Phase 3)

   - Shopify API integration
   - Meta Ads integration
   - TikTok Ads integration
   - Background jobs setup

3. **Week 3: AI & Intelligence** (Phase 4)

   - Claude AI integration
   - Recommendation engine
   - Test with sample data

4. **Week 4: Multi-Tenancy** (Phase 5)

   - Implement RLS policies
   - Tenant context middleware
   - Observability service

5. **Week 5: n8n Automation** (Phase 6)

   - Setup n8n instance
   - Create workflows
   - Webhook integration
   - Testing

6. **Week 6: Deployment** (Phase 7)
   - Docker containerization
   - GCP Cloud Run deployment
   - Monitoring setup
   - Production hardening

---

## Testing & Validation

### Unit Tests

```bash
pytest backend/tests/unit -v
```

### Integration Tests

```bash
pytest backend/tests/integration -v
```

### End-to-End Tests

```bash
pytest backend/tests/e2e -v
```

### Load Testing

```bash
locust -f scripts/load_test.py --headless -u 1000 -r 100
```

---

## Troubleshooting

### Issue: "Tenant not found"

**Solution:** Check JWT token contains `tenant_id` and tenant exists in database

### Issue: "RLS policy denies access"

**Solution:** Verify `SET app.tenant_id` is set before query

### Issue: "n8n webhook timeout"

**Solution:** Check Pulsepoint API is running and accessible from n8n

### Issue: "Claude API rate limited"

**Solution:** Implement exponential backoff in Anthropic service

---

## Documentation References

- [IMPLEMENTATION_GUIDE.md](./IMPLEMENTATION_GUIDE.md) - Detailed code implementation
- [DETAILED_APPROACH.md](./DETAILED_APPROACH.md) - Multi-tenancy & observability deep dive
- [ARCHITECTURE_DECISIONS.md](./ARCHITECTURE_DECISIONS.md) - Why we chose these technologies
- [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) - API endpoints and database schema

---

## Support & Community

- **GitHub Issues:** Report bugs and request features
- **Documentation:** Complete API reference in `/docs`
- **Email:** support@pulsepoint.com

---

**Version:** 1.0.0 | **Last Updated:** January 2026 | **Status:** Production Ready

---

## Quick Start

### Prerequisites

- Python 3.11+
- PostgreSQL 15+
- Google Cloud Project (BigQuery, Pub/Sub, Cloud Run)
- API Keys: Shopify, Meta, TikTok, Google Ads, Anthropic (Claude)

### Installation

```bash
# Clone repository
git clone https://github.com/yourcompany/pulsepoint.git
cd pulsepoint

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup environment variables
cp .env.example .env
# Edit .env with your API keys and database URLs

# Run database migrations
alembic upgrade head

# Start MCP server
uvicorn backend.api.main:app --reload
```

Access API documentation at `http://localhost:8000/docs`

---

## Architecture

### High-Level Flow

```
E-Commerce Platform                Advertising Platforms
        â”‚                                    â”‚
    Shopify â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
(Orders, Inventory)â”‚          â”‚             â”‚                â”‚
                   â”‚       Meta        TikTok           Google
                   â”‚       Ads          Ads              Ads
                   â”‚          â”‚             â”‚                â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ MCP Server       â”‚
                    â”‚ (Pulsepoint)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚                 â”‚
           â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Attributionâ”‚    â”‚AI Engine â”‚    â”‚Observabilityâ”‚
    â”‚Service    â”‚    â”‚(Claude)  â”‚    â”‚Service      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                 â”‚                 â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚Dashboard/Alerts â”‚
                    â”‚Recommendations  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

#### 1. **Data Integration Layer**

- **ShopifyService**: Syncs products, orders, inventory every hour
- **MetaAdsService**: Fetches campaign performance by geography
- **TikTokAdsService**: Aggregates TikTok metrics
- **GoogleAdsService**: Queries Google Ads via GAQL
- **ScheduledETLJobs**: Celery tasks run async data syncs

#### 2. **Data Unification**

- **AttributionService**: Matches orders to campaigns using last-touch attribution
- **InventoryIntelligenceService**: Calculates days of cover, stock issues
- **UnifiedPerformanceModel**: Combines all data in BigQuery

#### 3. **AI Intelligence**

- **AIInsightsService**: Uses Claude to analyze data and generate summaries
- **RecommendationEngine**: Creates prioritized budget allocation recommendations
- **AlertDetectionService**: Triggers urgent alerts (stockouts, ROAS drops)

#### 4. **API & Frontend**

- **REST API**: FastAPI endpoints for all operations
- **Natural Language Q&A**: `POST /api/ask` - Ask questions, get AI answers
- **Web Dashboard**: React frontend with real-time updates

#### 5. **Observability**

- **ObservabilityService**: Tracks all events (questions, costs, accuracy)
- **TokenTracker**: Monitors AI API usage for billing
- **QuestionTracker**: Logs questions â†’ answers â†’ user feedback
- **Metrics**: Prometheus for real-time dashboards

---

## Multi-Tenant Architecture

### Data Isolation Strategy

Pulsepoint implements **4-layer tenant isolation**:

#### Layer 1: Database Row-Level Security (PostgreSQL)

```sql
-- Users can only see rows where tenant_id matches their token
ALTER TABLE brands ENABLE ROW LEVEL SECURITY;
CREATE POLICY brand_isolation ON brands
    USING (tenant_id = current_setting('app.tenant_id')::uuid);
```

#### Layer 2: Schema Isolation (Optional)

```python
# For extreme isolation: separate schema per tenant
def get_db_session(tenant_id: str):
    session = SessionLocal()
    session.execute(f"SET search_path TO tenant_{tenant_id}")
    return session
```

#### Layer 3: BigQuery Dataset Isolation

```python
# Each tenant gets own BigQuery dataset
dataset_id = f"pulsepoint_tenant_{tenant_id}"
# Access controlled via IAM
```

#### Layer 4: Cache Isolation (Redis)

```python
# All cache keys namespaced
cache_key = f"tenant:{tenant_id}:dashboard:{brand_id}"
```

### Tenant Context Middleware

Every request carries tenant context in JWT token:

```python
# Automatic tenant enforcement
@app.middleware("http")
async def tenant_context_middleware(request: Request, call_next):
    token = request.headers.get("Authorization")
    payload = jwt.decode(token, SECRET_KEY)

    # Set thread-local context
    current_tenant_id.set(payload["tenant_id"])
    current_user_id.set(payload["user_id"])

    response = await call_next(request)
    return response
```

**Result:** No tenant can access another tenant's data, guaranteed at database level.

---

## Observability & Monitoring

### What We Track

#### 1. **Question Tracking**

```python
# Every AI question logged with metadata
{
    "question_id": "q_123",
    "tenant_id": "t_456",
    "user_id": "u_789",
    "question": "Which products performed best last month?",
    "asked_at": "2024-01-15T10:30:00Z",
    "accuracy_score": 0.92  # Updated after user feedback
}
```

#### 2. **Token & Cost Tracking**

```python
# Every API call to Claude tracked
{
    "operation": "generate_dashboard_summary",
    "model": "claude-3-sonnet",
    "input_tokens": 5000,
    "output_tokens": 1200,
    "total_tokens": 6200,
    "cost_usd": 0.0234  # ~$3 per million input, $15 per million output
}
```

#### 3. **Accuracy Scoring**

```python
# User feedback improves model understanding
{
    "question_id": "q_123",
    "user_rating": 5,  # 1-5 stars
    "was_actionable": true,
    "accuracy_comment": "Perfect insight, exactly what I needed"
}
```

#### 4. **API Latency**

```python
# Track endpoint performance
{
    "endpoint": "/api/brands/{id}/dashboard",
    "p50_latency_ms": 250,
    "p95_latency_ms": 850,
    "p99_latency_ms": 2100,
    "error_rate": 0.001
}
```

#### 5. **Real-Time Alerts**

```yaml
# Alert if:
- Token usage > $1000/day per tenant
- API error rate > 5%
- AI accuracy score < 70%
- Data sync failures
```

### Observability Queries

```sql
-- Token usage by tenant (daily)
SELECT
    tenant_id,
    DATE(recorded_at) as date,
    SUM(total_tokens) as total_tokens,
    SUM(total_cost_usd) as cost_usd
FROM ai_token_usage
WHERE DATE(recorded_at) >= CURRENT_DATE() - 30
GROUP BY tenant_id, date
ORDER BY date DESC;

-- Question accuracy by model
SELECT
    model,
    COUNT(*) as questions,
    AVG(accuracy_score) as avg_accuracy,
    STDDEV(accuracy_score) as accuracy_std
FROM ai_questions q
JOIN ai_feedback f ON q.question_id = f.question_id
GROUP BY model;

-- Latency by endpoint
SELECT
    endpoint,
    COUNT(*) as requests,
    PERCENTILE_CONT(duration_ms, 0.95) as p95_latency_ms
FROM observability_events
WHERE event_type = 'api.response'
GROUP BY endpoint
ORDER BY p95_latency_ms DESC;
```

### Real-Time Dashboards

**Metrics Dashboard (Grafana):**

- Requests per second
- Average API latency (p50, p95, p99)
- Error rate (%)
- AI tokens consumed (by model, by tenant)
- Cost per operation

**Observability Dashboard (BigQuery):**

- Questions asked (by category, by user)
- Accuracy trends (by model, by week)
- Expensive operations (highest token usage)
- User satisfaction (rating distribution)

---

## API Reference

### Authentication

All requests require Bearer token in `Authorization` header:

```bash
curl -H "Authorization: Bearer YOUR_JWT_TOKEN" \
     https://api.pulsepoint.com/api/brands/brand_123/dashboard
```

### Core Endpoints

#### 1. **Get Dashboard Summary**

```
GET /api/brands/{brand_id}/dashboard

Response:
{
  "summary": {
    "summary": "Last week you spent $18,500 on ads and generated $72,000 in Shopify revenue at a blended 3.9x ROAS...",
    "channel_performance": {
      "meta": "Stable performer at 4.2x ROAS...",
      "tiktok": "ROAS declined 25% week-over-week...",
      "google": "Showing improvement..."
    },
    "inventory_insights": "You are heavily promoting SKU-A on TikTok in CA/TX, but inventory is low..."
  },
  "recommendations": [
    {
      "priority": "high",
      "action": "Cut TikTok spend on SKU-A in CA/TX by $1,000",
      "reason": "Low ROAS (2.1x) + low stock (8 days)...",
      "expected_impact": "Avoid stockouts, improve ROAS by 0.3x"
    }
  ],
  "performance_snapshot": {
    "total_spend": 18500,
    "total_revenue": 72000,
    "blended_roas": 3.9,
    "by_channel": {...}
  }
}
```

#### 2. **Ask Question (Natural Language Q&A)**

```
POST /api/brands/{brand_id}/ask

Request:
{
  "question": "Which regions are performing best this month?"
}

Response:
{
  "question_id": "q_12345",
  "answer": "Your top 3 regions are: 1) California with 4.5x ROAS, 2) Texas with 4.2x ROAS, 3) New York with 3.8x ROAS. California accounts for 45% of revenue despite being only 28% of spend.",
  "sources": [
    "unified_performance_table",
    "ad_performance_meta",
    "orders_shopify"
  ],
  "tokens_used": 1250,
  "generation_time_ms": 1420,
  "confidence_score": 0.95
}
```

**Tracked for observability:**

- Question logged to `ai_questions` table
- Tokens billed to tenant
- User can provide feedback (1-5 stars) to improve accuracy scoring

#### 3. **Get Alerts**

```
GET /api/brands/{brand_id}/alerts

Response:
{
  "alerts": [
    {
      "type": "roas_drop",
      "severity": "high",
      "channel": "tiktok",
      "title": "TikTok ROAS dropped 25% in CA/TX",
      "explanation": "The decline is driven by new broad prospecting campaign with low conversion rates...",
      "recommended_action": "Pause Campaign 789 in CA/TX or adjust targeting"
    },
    {
      "type": "stockout_risk",
      "severity": "critical",
      "product": "SKU-A",
      "title": "SKU-A will stock out in 3 days",
      "explanation": "Current sales velocity is 15 units/day with 45 units remaining...",
      "recommended_action": "Pause all ads promoting SKU-A immediately"
    }
  ]
}
```

#### 4. **Get Recommendations**

```
GET /api/brands/{brand_id}/recommendations?limit=10&sort_by=priority

Response:
{
  "recommendations": [
    {
      "recommendation_id": "rec_123",
      "type": "reduce_spend",
      "priority": "high",
      "action": "Cut TikTok spend on SKU-A in CA/TX by $1,000",
      "current_spend": 5000,
      "recommended_change": -1000,
      "reason": "ROAS 2.1x (below 2.5 target) + 8 days inventory cover (critically low)",
      "expected_impact": "Avoid stockouts, improve blended ROAS by 0.3x"
    }
  ]
}
```

#### 5. **Connect Integration**

```
POST /api/brands/{brand_id}/integrations

Request:
{
  "platform": "shopify",
  "oauth_code": "abc123xyz"
}

Response:
{
  "status": "connecting",
  "platform": "shopify",
  "syncing": true,
  "sync_started_at": "2024-01-15T10:30:00Z"
}
```

**Background:** Triggers async job to sync Shopify data. Customer can check status later.

#### 6. **Export Data**

```
GET /api/brands/{brand_id}/export?format=csv&start_date=2024-01-01&end_date=2024-01-15

Response:
{
  "download_url": "https://storage.googleapis.com/pulsepoint-exports/brand_123/report_2024_01_15.csv",
  "expires_at": "2024-01-16T10:30:00Z",
  "format": "csv",
  "rows": 15234
}
```

**Tracked:** Data export logged for PII handling and compliance.

#### 7. **Admin: Get Tenant Usage**

```
GET /mcp/admin/v1/tenants/{tenant_id}/usage

Response:
{
  "tenant_id": "t_123",
  "total_questions": 1245,
  "total_tokens_consumed": 2500000,
  "total_cost_usd": 7.50,
  "by_date": [
    {
      "date": "2024-01-15",
      "questions": 42,
      "tokens": 85000,
      "cost": 0.26
    }
  ],
  "by_operation": {
    "dashboard_summary": 45000,
    "ask_question": 25000,
    "alerts": 10000
  }
}
```

---

## Configuration

### Environment Variables

```bash
# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/pulsepoint
BIGQUERY_PROJECT_ID=your-gcp-project

# Redis
REDIS_URL=redis://localhost:6379

# External APIs
SHOPIFY_CLIENT_ID=your_shopify_id
SHOPIFY_CLIENT_SECRET=your_shopify_secret

META_APP_ID=your_meta_id
META_APP_SECRET=your_meta_secret

TIKTOK_CLIENT_ID=your_tiktok_id
TIKTOK_CLIENT_SECRET=your_tiktok_secret

GOOGLE_DEVELOPER_TOKEN=your_google_token
GOOGLE_CLIENT_ID=your_google_id
GOOGLE_CLIENT_SECRET=your_google_secret

# Claude API
ANTHROPIC_API_KEY=sk-ant-...

# JWT
SECRET_KEY=your-secret-key-here

# Pub/Sub for observability
PUBSUB_PROJECT_ID=your-gcp-project

# Feature Flags
DEBUG=False
LOG_LEVEL=INFO
```

### Docker Deployment

```bash
# Build image
docker build -t pulsepoint:latest .

# Run locally with compose
docker-compose up

# Deploy to Cloud Run
gcloud run deploy pulsepoint \
  --image gcr.io/your-project/pulsepoint \
  --platform managed \
  --region us-central1 \
  --set-env-vars DATABASE_URL=$DATABASE_URL,ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY
```

---

## Security Best Practices

### Multi-Tenant Isolation

âœ… **Row-Level Security:** PostgreSQL RLS enforces tenant boundaries
âœ… **JWT Context:** Every request includes tenant_id, validated server-side
âœ… **Database Policies:** Queries automatically filtered by tenant
âœ… **API Authorization:** RBAC prevents unauthorized data access

### Data Protection

âœ… **Encryption at Rest:** OAuth tokens encrypted in database
âœ… **Encryption in Transit:** HTTPS/TLS 1.3 enforced
âœ… **Secret Management:** All keys in Google Secret Manager
âœ… **Backups:** Daily automated backups with point-in-time recovery

### API Security

âœ… **Rate Limiting:** 100 req/min per IP, 20 req/min per AI question
âœ… **Input Validation:** Pydantic models sanitize all inputs
âœ… **SQL Injection Prevention:** Parameterized queries only
âœ… **CORS:** Whitelist specific origins only

### Monitoring & Compliance

âœ… **Audit Logging:** All operations logged with user/timestamp
âœ… **SOC 2 Controls:** Documented security procedures
âœ… **GDPR/CCPA:** Data residency and deletion policies
âœ… **Alerts:** Suspicious activity triggers immediate notification

---

## Database Schema

### Core Tables

| Table                  | Purpose                        | Tenant Isolation |
| ---------------------- | ------------------------------ | ---------------- |
| `tenants`              | Organizations using Pulsepoint | N/A (root)       |
| `users`                | Tenant users with roles        | tenant_id        |
| `brands`               | E-commerce stores per tenant   | tenant_id        |
| `integrations`         | OAuth credentials (encrypted)  | tenant_id        |
| `observability_events` | All trackable events           | tenant_id        |
| `ai_questions`         | Questions asked by users       | tenant_id        |
| `ai_answers`           | AI responses                   | question_id (FK) |
| `ai_feedback`          | User ratings on answers        | question_id (FK) |
| `ai_token_usage`       | Token consumption tracking     | tenant_id        |

### BigQuery Analytics Tables

| Dataset                  | Table                  | Purpose                           |
| ------------------------ | ---------------------- | --------------------------------- |
| `pulsepoint_tenant_{id}` | `unified_performance`  | Orders + ads + inventory joined   |
| `pulsepoint_tenant_{id}` | `ai_insights`          | Cached AI summaries               |
| `pulsepoint_tenant_{id}` | `ad_performance`       | Multi-platform ad metrics         |
| `pulsepoint_tenant_{id}` | `inventory_levels`     | Current stock by product/location |
| `pulsepoint`             | `observability_events` | System-wide event log             |

---

## Deployment Guide

### Local Development

```bash
# 1. Start services
docker-compose up

# 2. Run migrations
docker exec pulsepoint-api alembic upgrade head

# 3. Seed sample data (optional)
docker exec pulsepoint-api python scripts/seed_data.py

# 4. Access dashboard
open http://localhost:3000

# 5. Access API docs
open http://localhost:8000/docs
```

### Production on GCP

```bash
# 1. Set up GCP project
gcloud projects create pulsepoint-prod
gcloud config set project pulsepoint-prod

# 2. Create databases
gcloud sql instances create pulsepoint-db \
  --database-version=POSTGRES_15 \
  --tier=db-custom-2-8192 \
  --region=us-central1

# 3. Build and push image
gcloud builds submit --tag gcr.io/pulsepoint-prod/api

# 4. Deploy to Cloud Run
gcloud run deploy pulsepoint-api \
  --image gcr.io/pulsepoint-prod/api \
  --platform managed \
  --region us-central1 \
  --set-env-vars DATABASE_URL=$DATABASE_URL

# 5. Configure monitoring
gcloud monitoring dashboards create --config-from-file=monitoring/dashboards.yaml

# 6. Set up alerting
gcloud alpha monitoring policies create --policy-from-file=monitoring/alerts.yaml
```

### High Availability

```yaml
# CloudSQL: Multi-region failover
instance:
  tier: db-custom-4-16384
  availability_type: REGIONAL
  backup_configuration:
    enabled: true
    location: us # Multi-region backup

# Cloud Run: Auto-scaling
max_instances: 100
min_instances: 2

# Load Balancer: Global distribution
frontend_config:
  load_balancing_scheme: EXTERNAL
  protocol: HTTPS
```

---

## Monitoring & Observability

### Key Metrics

```bash
# Token usage (cost control)
rate(ai_tokens_consumed_total{tenant="t_123"}[1h])

# API latency (performance)
histogram_quantile(0.95, rate(api_latency_ms_bucket[5m]))

# Error rate (reliability)
rate(api_errors_total[5m]) > 0.05

# Question accuracy (quality)
avg(ai_question_accuracy_score)

# Data sync status (freshness)
rate(data_sync_failures_total[1h]) > 0
```

### Dashboards Available

1. **Executive Dashboard:** Revenue, ROAS, recommendations accepted
2. **Operations Dashboard:** Data sync status, integrations health
3. **AI Dashboard:** Token usage, question accuracy, model performance
4. **Security Dashboard:** Failed auth attempts, suspicious activity
5. **Tenant Dashboard:** Per-tenant billing and usage

---

## Troubleshooting

### Common Issues

**Issue: "Access denied. Brand does not exist or belongs to another tenant."**

- âœ… Verify JWT token contains correct tenant_id
- âœ… Verify brand exists and belongs to tenant
- âœ… Check RLS policies are enabled

**Issue: High AI token usage / unexpected costs**

- âœ… Check `ai_token_usage` table for expensive operations
- âœ… Review prompts for optimization opportunities
- âœ… Set per-tenant token usage limits

**Issue: Slow API responses**

- âœ… Check `/health` endpoint for database connectivity
- âœ… Review slow queries in PostgreSQL logs
- âœ… Verify BigQuery cache hit rates

**Issue: Data not syncing from Shopify**

- âœ… Verify OAuth token not expired
- âœ… Check integration.last_synced_at timestamp
- âœ… Review sync_error_message in integrations table

---

## Support & Contributing

### Getting Help

- ğŸ“– [Documentation](https://docs.pulsepoint.com)
- ğŸ’¬ [Discord Community](https://discord.gg/pulsepoint)
- ğŸ› [Issue Tracker](https://github.com/pulsepoint/pulsepoint/issues)
- ğŸ“§ support@pulsepoint.com

### Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

---

## License

Pulsepoint is licensed under the Apache 2.0 License. See [LICENSE](LICENSE) file for details.

---

## Roadmap

### Q1 2024

- [ ] Multi-language support (Spanish, French, German)
- [ ] Custom attribution models (first-touch, time-decay)
- [ ] Inventory forecasting (ML-based demand prediction)

### Q2 2024

- [ ] Mobile app for on-the-go insights
- [ ] Shopify app marketplace integration
- [ ] Advanced segmentation engine

### Q3 2024

- [ ] Predictive ROAS models
- [ ] Competitive analysis integration
- [ ] Custom recommendation rules builder

---

## Acknowledgments

Built with:

- ğŸ¦™ **Claude AI** (Anthropic) - Intelligence layer
- ğŸ˜ **PostgreSQL** - Transactional database
- ğŸ“Š **BigQuery** - Analytics engine
- âš¡ **FastAPI** - API framework
- â˜ï¸ **Google Cloud** - Infrastructure

---

**Questions? Start a discussion in [Discussions](https://github.com/pulsepoint/pulsepoint/discussions)**
